optimizer : {
  type: AdamW,
  kwargs: {
  lr : 0.0005, 
  weight_decay : 0.05
}}

scheduler: {
  type: CosLR,
  kwargs: {
    epochs: 300,
    initial_epochs : 10
}}

dataset : {
  train : { _base_: configs/dataset_configs/ModelNet40.yaml, 
            others: {subset: 'train'}},
  val : { _base_: configs/dataset_configs/ModelNet40.yaml, 
            others: {subset: 'test'}},
  test : { _base_: configs/dataset_configs/ModelNet40.yaml, 
            others: {subset: 'test'}}}
model : {
  NAME: PointMamba,
  trans_dim: 384, 
  depth: 1, 
  drop_path_rate: 0.1, 
  cls_dim: 40, 
  num_heads: 6,
  group_size: 32, 
  num_group: 512, 
  encoder_dims: 256,

  use_llama: True,
  llama_path: checkpoints/llama,
  llama_cfg: {
      dim: 4096,
      multiple_of: 256,
      n_heads: 32,
      n_layers: 32,
      norm_eps: 1.0e-6,
      vocab_size: -1,
      first_layer: 31
  },
  only_mlp: False,
  use_vgg: False,
}
npoints: 8192
total_bs : 1
step_per_update : 1
max_epoch : 300
grad_norm_clip : 10


consider_metric: CDL1